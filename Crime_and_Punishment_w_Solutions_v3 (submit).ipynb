{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Crime_and_Punishment_w_Solutions_v3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hHq3xDQRCwt"
      },
      "source": [
        "# Run each time you're on a new Google CoLab setup or \n",
        "! pip install nltk\n",
        "! pip install bs4\n",
        "! pip install requests\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('book')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm66yOQBRCwy"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uyY9H1TRCw1"
      },
      "source": [
        "text = requests.get(\"http://inta.gatech.s3.amazonaws.com/crime_and_punishment.txt\").text\n",
        "# text = requests.get(\"http://www.gutenberg.org/files/2554/2554.txt\").content\n",
        "print(text[0:100]) # Here are the first 100 characters of Project Gutenberg's crime and punishment\n",
        "print('length of text: %s characters' % len(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ADGjDJYRCw4"
      },
      "source": [
        "tokens = nltk.word_tokenize(text)\n",
        "# print the number of word \"tokens\" in the book:\n",
        "print('There are %s words in crime and punishment' % len(tokens))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uKXBpmXRCw7"
      },
      "source": [
        "text = nltk.Text(tokens)\n",
        "print(text)\n",
        "# Create an NLTK text object, in case you want to use some of the nltk.text functions\n",
        "# http://www.nltk.org/api/nltk.html#module-nltk.text \n",
        "print(text[500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofThN0YbRCw_"
      },
      "source": [
        "# Do word counts using the defaultdict construction in python\n",
        "# Defaultdicts are dicts where looking for a missing key gives a default value instead of an error\n",
        "# An int-based defaultdict would return a zero-integer. So starting with a defaultdict and \n",
        "# incrementing hte value every time a word occurs will give a word count\n",
        "from collections import defaultdict\n",
        "counts=defaultdict(int)\n",
        "for word in text:\n",
        "    counts[word]+=1\n",
        "print('the word \"%s\" appeared %s times' % ('the',counts['the']))\n",
        "# To sort the wordcount, do:\n",
        "sorted_counts=sorted(counts.items(), key=lambda x: -x[1])\n",
        "print('the word \"%s\" is the most common, appearing %s times' % (sorted_counts[0][0], sorted_counts[0][1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tExn9yWCRCxB"
      },
      "source": [
        "# An alternative way to do word counts is the \"Counter\" object\n",
        "from collections import Counter\n",
        "counts = Counter(text)\n",
        "print('the word \"%s\" appeared %s times' % ('the',counts['the']))\n",
        "# If you print \"counts\", it will print the sorted wordcount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ohllc08RCxE"
      },
      "source": [
        "#How many unique words are there?\n",
        "#This should be the length of the word count dictionary\n",
        "print('There are %s unique words' % len(sorted_counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YXaiQZKRCxH"
      },
      "source": [
        "# To figure out how many of these words are English words, we need an english dictionary\n",
        "english=nltk.corpus.words.words('en')\n",
        "\n",
        "print(english[0:15])\n",
        "# load an english dictionary (i.e. list of english words) from nltk\n",
        "\n",
        "english = list(set([w.lower() for w in english]))\n",
        "# Then make sure all words are lowercase - so we would accept 'aaron' as a word, no matter the capitalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnNw0RYLRCxK"
      },
      "source": [
        "es=set(english)\n",
        "ws=set(counts.keys())\n",
        "overlap = ws.intersection(es)\n",
        "missing = ws.difference(es)\n",
        "print('%s of the words in C&P are in the English dictionary, out of %s' % (len(overlap),len(ws)))\n",
        "print('A few c&p words missing from the english dictionary: %s' % str(list(missing)[0:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvfSv6Y1RCxM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "12c67d2d-0b58-4036-b85c-61b11320e0fe"
      },
      "source": [
        "from nltk.book import *\n",
        "#########################\n",
        "##### My Change begin\n",
        "#########################\n",
        "import string\n",
        "text1\n",
        "text1.concordance(\"test\")\n",
        "print(\"=================\")\n",
        "print(type(text1))\n",
        "print(\"=================\")\n",
        "text1.similar(\"test\")\n",
        "print(\"=================\")\n",
        "text1.common_contexts([\"test\", \"the\"])\n",
        "print(\"=================\")\n",
        "f1 = FreqDist(text1)\n",
        "print(f1)\n",
        "print(type(text1))\n",
        "print(f1.most_common(10))\n",
        "#########################\n",
        "#####  My Change end\n",
        "#########################"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 4 of 4 matches:\n",
            "s . So Jonah ' s Captain prepares to test the length of Jonah ' s purse , ere \n",
            "ions of him , and at last , fully to test the truth , by referring the whole m\n",
            " At any rate -- though indeed such a test at such a time might be deceptive --\n",
            "on authorities you can refer to , to test my accuracy . There is a Leviathanic\n",
            "=================\n",
            "<class 'nltk.text.Text'>\n",
            "=================\n",
            "which point stop notice by and see him all it you take hand them what\n",
            "be have eye view whom\n",
            "=================\n",
            "No common contexts were found\n",
            "=================\n",
            "<FreqDist with 19317 samples and 260819 outcomes>\n",
            "<class 'nltk.text.Text'>\n",
            "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rpJgvtcRCxO"
      },
      "source": [
        "from collections import Counter\n",
        "b=Counter(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGn7L-AzRCxQ"
      },
      "source": [
        "Below this line, we're dealing with data downloaded from a website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr8hx11LRCxR"
      },
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "url_response_object=requests.get(url)\n",
        "url_response_object.raise_for_status()\n",
        "html = url_response_object.content\n",
        "print(html[0:100]) # Here's what some of the html looks like"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tRREnKKRCxT"
      },
      "source": [
        "raw = BeautifulSoup(html).get_text()\n",
        "print(raw[0:100]) # Here's what the text looks like, after removing these html tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foojjoYeRCxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a267f8ca-a86d-4bb8-dcf6-c08a4faef680"
      },
      "source": [
        "raw[1:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nBBC NEWS | Health | Blondes 'to die out in 200 years'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNEWS\\n\\xa0\\xa0S\""
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n0yqEqCRCxX"
      },
      "source": [
        "tokens = nltk.word_tokenize(raw)\n",
        "print(tokens[0:15]) # Here we've split things out by whitespace into a list of \"tokens\" or words/punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSLDcZdiRCxZ"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OpYV6fjRCxb"
      },
      "source": [
        "tokens[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhMmFYgjRkHF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}